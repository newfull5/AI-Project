{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "rng = np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters.\n",
    "learning_rate = 0.01\n",
    "training_steps = 5000\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data.\n",
    "X = np.array([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "              7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "Y = np.array([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "              2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight and Bias, initialized randomly.\n",
    "W = tf.Variable(rng.randn(), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")\n",
    "\n",
    "# Linear regression (Wx + b).\n",
    "def linear_regression(x):\n",
    "    return W * x + b\n",
    "\n",
    "# Mean square error.\n",
    "def mean_square(y_pred, y_true):\n",
    "    return tf.reduce_sum(tf.pow(y_pred-y_true, 2)) / (2 * n_samples)\n",
    "\n",
    "# Stochastic Gradient Descent Optimizer.\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. \n",
    "def run_optimization():\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation.\n",
    "    with tf.GradientTape() as g:\n",
    "        pred = linear_regression(X)\n",
    "        loss = mean_square(pred, Y)\n",
    "\n",
    "    # Compute gradients.\n",
    "    gradients = g.gradient(loss, [W, b])\n",
    "    \n",
    "    # Update W and b following gradients.\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50, loss: 0.087790, W: 0.310720, b: 0.379916\n",
      "step: 100, loss: 0.086548, W: 0.307238, b: 0.404597\n",
      "step: 150, loss: 0.085448, W: 0.303962, b: 0.427824\n",
      "step: 200, loss: 0.084473, W: 0.300879, b: 0.449682\n",
      "step: 250, loss: 0.083610, W: 0.297978, b: 0.470253\n",
      "step: 300, loss: 0.082846, W: 0.295247, b: 0.489611\n",
      "step: 350, loss: 0.082170, W: 0.292677, b: 0.507829\n",
      "step: 400, loss: 0.081570, W: 0.290259, b: 0.524973\n",
      "step: 450, loss: 0.081039, W: 0.287983, b: 0.541108\n",
      "step: 500, loss: 0.080569, W: 0.285842, b: 0.556291\n",
      "step: 550, loss: 0.080153, W: 0.283826, b: 0.570580\n",
      "step: 600, loss: 0.079784, W: 0.281929, b: 0.584027\n",
      "step: 650, loss: 0.079458, W: 0.280144, b: 0.596682\n",
      "step: 700, loss: 0.079168, W: 0.278465, b: 0.608591\n",
      "step: 750, loss: 0.078912, W: 0.276884, b: 0.619798\n",
      "step: 800, loss: 0.078685, W: 0.275396, b: 0.630345\n",
      "step: 850, loss: 0.078484, W: 0.273996, b: 0.640271\n",
      "step: 900, loss: 0.078307, W: 0.272679, b: 0.649612\n",
      "step: 950, loss: 0.078149, W: 0.271439, b: 0.658402\n",
      "step: 1000, loss: 0.078009, W: 0.270272, b: 0.666675\n",
      "step: 1050, loss: 0.077886, W: 0.269174, b: 0.674460\n",
      "step: 1100, loss: 0.077776, W: 0.268140, b: 0.681786\n",
      "step: 1150, loss: 0.077679, W: 0.267168, b: 0.688681\n",
      "step: 1200, loss: 0.077594, W: 0.266253, b: 0.695169\n",
      "step: 1250, loss: 0.077518, W: 0.265391, b: 0.701275\n",
      "step: 1300, loss: 0.077450, W: 0.264581, b: 0.707021\n",
      "step: 1350, loss: 0.077391, W: 0.263818, b: 0.712429\n",
      "step: 1400, loss: 0.077338, W: 0.263100, b: 0.717518\n",
      "step: 1450, loss: 0.077291, W: 0.262425, b: 0.722307\n",
      "step: 1500, loss: 0.077250, W: 0.261789, b: 0.726815\n",
      "step: 1550, loss: 0.077213, W: 0.261191, b: 0.731056\n",
      "step: 1600, loss: 0.077180, W: 0.260628, b: 0.735048\n",
      "step: 1650, loss: 0.077152, W: 0.260098, b: 0.738804\n",
      "step: 1700, loss: 0.077126, W: 0.259599, b: 0.742339\n",
      "step: 1750, loss: 0.077104, W: 0.259130, b: 0.745666\n",
      "step: 1800, loss: 0.077084, W: 0.258688, b: 0.748797\n",
      "step: 1850, loss: 0.077066, W: 0.258273, b: 0.751743\n",
      "step: 1900, loss: 0.077050, W: 0.257881, b: 0.754516\n",
      "step: 1950, loss: 0.077036, W: 0.257513, b: 0.757125\n",
      "step: 2000, loss: 0.077024, W: 0.257167, b: 0.759580\n",
      "step: 2050, loss: 0.077013, W: 0.256841, b: 0.761891\n",
      "step: 2100, loss: 0.077004, W: 0.256534, b: 0.764066\n",
      "step: 2150, loss: 0.076995, W: 0.256246, b: 0.766113\n",
      "step: 2200, loss: 0.076987, W: 0.255974, b: 0.768039\n",
      "step: 2250, loss: 0.076981, W: 0.255718, b: 0.769851\n",
      "step: 2300, loss: 0.076975, W: 0.255478, b: 0.771557\n",
      "step: 2350, loss: 0.076970, W: 0.255251, b: 0.773162\n",
      "step: 2400, loss: 0.076965, W: 0.255038, b: 0.774673\n",
      "step: 2450, loss: 0.076961, W: 0.254838, b: 0.776095\n",
      "step: 2500, loss: 0.076957, W: 0.254649, b: 0.777433\n",
      "step: 2550, loss: 0.076954, W: 0.254471, b: 0.778692\n",
      "step: 2600, loss: 0.076951, W: 0.254304, b: 0.779877\n",
      "step: 2650, loss: 0.076948, W: 0.254147, b: 0.780992\n",
      "step: 2700, loss: 0.076946, W: 0.253999, b: 0.782041\n",
      "step: 2750, loss: 0.076944, W: 0.253860, b: 0.783029\n",
      "step: 2800, loss: 0.076942, W: 0.253729, b: 0.783958\n",
      "step: 2850, loss: 0.076941, W: 0.253605, b: 0.784833\n",
      "step: 2900, loss: 0.076940, W: 0.253489, b: 0.785656\n",
      "step: 2950, loss: 0.076938, W: 0.253380, b: 0.786430\n",
      "step: 3000, loss: 0.076937, W: 0.253277, b: 0.787159\n",
      "step: 3050, loss: 0.076936, W: 0.253180, b: 0.787845\n",
      "step: 3100, loss: 0.076935, W: 0.253089, b: 0.788491\n",
      "step: 3150, loss: 0.076935, W: 0.253004, b: 0.789098\n",
      "step: 3200, loss: 0.076934, W: 0.252923, b: 0.789670\n",
      "step: 3250, loss: 0.076933, W: 0.252847, b: 0.790208\n",
      "step: 3300, loss: 0.076933, W: 0.252776, b: 0.790714\n",
      "step: 3350, loss: 0.076932, W: 0.252708, b: 0.791191\n",
      "step: 3400, loss: 0.076932, W: 0.252645, b: 0.791639\n",
      "step: 3450, loss: 0.076932, W: 0.252586, b: 0.792061\n",
      "step: 3500, loss: 0.076931, W: 0.252530, b: 0.792458\n",
      "step: 3550, loss: 0.076931, W: 0.252477, b: 0.792832\n",
      "step: 3600, loss: 0.076931, W: 0.252427, b: 0.793184\n",
      "step: 3650, loss: 0.076931, W: 0.252381, b: 0.793514\n",
      "step: 3700, loss: 0.076930, W: 0.252337, b: 0.793826\n",
      "step: 3750, loss: 0.076930, W: 0.252295, b: 0.794119\n",
      "step: 3800, loss: 0.076930, W: 0.252256, b: 0.794395\n",
      "step: 3850, loss: 0.076930, W: 0.252220, b: 0.794654\n",
      "step: 3900, loss: 0.076930, W: 0.252185, b: 0.794899\n",
      "step: 3950, loss: 0.076930, W: 0.252153, b: 0.795129\n",
      "step: 4000, loss: 0.076930, W: 0.252122, b: 0.795345\n",
      "step: 4050, loss: 0.076930, W: 0.252094, b: 0.795549\n",
      "step: 4100, loss: 0.076929, W: 0.252067, b: 0.795740\n",
      "step: 4150, loss: 0.076929, W: 0.252041, b: 0.795921\n",
      "step: 4200, loss: 0.076929, W: 0.252017, b: 0.796090\n",
      "step: 4250, loss: 0.076929, W: 0.251995, b: 0.796250\n",
      "step: 4300, loss: 0.076929, W: 0.251974, b: 0.796400\n",
      "step: 4350, loss: 0.076929, W: 0.251954, b: 0.796542\n",
      "step: 4400, loss: 0.076929, W: 0.251935, b: 0.796675\n",
      "step: 4450, loss: 0.076929, W: 0.251917, b: 0.796800\n",
      "step: 4500, loss: 0.076929, W: 0.251901, b: 0.796918\n",
      "step: 4550, loss: 0.076929, W: 0.251885, b: 0.797029\n",
      "step: 4600, loss: 0.076929, W: 0.251870, b: 0.797133\n",
      "step: 4650, loss: 0.076929, W: 0.251856, b: 0.797232\n",
      "step: 4700, loss: 0.076929, W: 0.251843, b: 0.797324\n",
      "step: 4750, loss: 0.076929, W: 0.251831, b: 0.797411\n",
      "step: 4800, loss: 0.076929, W: 0.251819, b: 0.797493\n",
      "step: 4850, loss: 0.076929, W: 0.251809, b: 0.797570\n",
      "step: 4900, loss: 0.076929, W: 0.251798, b: 0.797643\n",
      "step: 4950, loss: 0.076929, W: 0.251789, b: 0.797711\n",
      "step: 5000, loss: 0.076929, W: 0.251780, b: 0.797775\n"
     ]
    }
   ],
   "source": [
    "# Run training for the given number of steps.\n",
    "for step in range(1, training_steps + 1):\n",
    "    # Run the optimization to update W and b values.\n",
    "    run_optimization()\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        pred = linear_regression(X)\n",
    "        loss = mean_square(pred, Y)\n",
    "        print(\"step: %i, loss: %f, W: %f, b: %f\" % (step, loss, W.numpy(), b.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=271880, shape=(), dtype=float32, numpy=6.1934547>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_square(linear_regression(X), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as g:\n",
    "    g.gradient(mean_square(linear_regression(X), Y), [W, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'weight:0' shape=() dtype=float32, numpy=-0.6426552>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
